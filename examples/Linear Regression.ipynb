{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression is a well known machine learning technique that aims to fit a function to input data and known outputs. The data set can be considered a matrix $X$ $\\in \\mathbb{R}^{n \\times d}$. This is to imply there are $n$ data points, each with a dimension $d$. We are also given ground truth outputs $y$ $\\in \\mathbb{R}^{n}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we wish to approximate a function $f$ such that $f(x_{i}) = y_{i}$, where $i$ ranges from 1 to $n$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When it comes to linear regression, the function looks as follows:\n",
    "\n",
    "$ \\hat{y} = X \\Theta$\n",
    "\n",
    "where the hat on the y indicates the given y outputs. The $X$ matrix is the given data set, and $\\Theta \\in \\mathbb{R}^{n}$ is a set of weights. Our goal is to find the best set of weights that fit our data best.\n",
    "\n",
    "In matrix form, we can see this even more clearly as\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{bmatrix}\n",
    "y_{1} \\\\\n",
    "y_{2} \\\\\n",
    "\\vdots \\\\\n",
    "y_{n} \\\\\n",
    "\\end{bmatrix}\n",
    " = \n",
    "\\begin{bmatrix}\n",
    "x_{1}^{T} \\\\\n",
    "x_{2}^{T} \\\\\n",
    "\\vdots \\\\\n",
    "x_{n}^{T} \n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\theta_{1} \\\\\n",
    "\\theta_{2} \\\\\n",
    "\\vdots \\\\\n",
    "\\theta_{n} \n",
    "\\end{bmatrix}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, there is no exact solution to this linear system. One way to solve this problem is to define a loss function and optimize it. A commone loss function for linear regression is to minimize the squared error from predicted outputs to ground truth outputs. \n",
    "\n",
    "$L(\\theta) = \\sum_{i=1}^{n} (x_{i}^T \\theta_{i} - y_{i})^{2} = ||X\\Theta - y||^{2}_{2}$\n",
    "\n",
    "This simply says, for each data vector and output, how far away is the prediction from the actual input value? The question now is how do we minimize this loss function. We want to solve for $\\Theta$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use some help from vector calculus to reduce the loss function to a solvable state. \n",
    "\n",
    "We can obtain a minimum solution $\\Theta$ when the gradient of the loss is equal to zero. \n",
    "\n",
    "$\\nabla{L(\\Theta^{*})} = 0$, where we've denoted here $\\Theta^{*}$ as the minimizing set of weight values. If our loss function is what is termed convex, we know that this local minimum is also the global minimum. In cases of non-convex functions, we are not guaranteed to have our local minima be the global minimum. Let's leave that issue for a later notebook. \n",
    "\n",
    "First, let us expand our loss function so we can easily take the gradient of it. \n",
    "\n",
    "\\begin{equation}\n",
    "L(\\Theta) = ||X\\Theta - y ||_{2}^{2} \\\\\n",
    "= (X \\Theta - y)^{T} (X \\Theta - y) \\\\\n",
    "= (\\Theta^{T} X^{T} - y^{T})(X \\Theta - y) \\\\\n",
    "= \\Theta^{T} X^{T} X \\Theta - 2 \\Theta^{T} X^{T} y - y^{T} y\\\\\n",
    "\\end{equation}\n",
    "\n",
    "Note, we have made use of the fact that $\\Theta^{T} X^{T} y = y^{T} X \\Theta$, as they are both scalar values, and the transpose of a scalar is the same scalar value. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we now have reorganized our loss function to look as follows: \n",
    "\n",
    "$L(\\Theta) = \\Theta^{T} X^{T} X \\Theta - 2 \\Theta^{T} X^{T} y - y^{T} y$. \n",
    "\n",
    "We want to take the gradient of this function with respect to the variable $\\Theta$. \n",
    "\n",
    "From vector calculus, we have: \n",
    "$\\nabla (a^T x) = a$ and $\\nabla (x^T A x) = (A + A^{T}) x$. \n",
    "\n",
    "Applying this to our problem, we find the gradient of the loss to be: \n",
    "\n",
    "$\\nabla L(\\Theta) = \\nabla (\\Theta^{T} X^{T} X \\Theta) - 2 \\nabla (\\Theta^{T} X^{T} y) - \\nabla(y^{T} y)$\n",
    "\n",
    "This gives\n",
    "\n",
    "$\\nabla L(\\Theta) = 2 X^{T} X \\Theta  - 2 X^{T} y$\n",
    "\n",
    "We want this value to be equal to zero to find our minimizing value for $\\Theta$. \n",
    "\n",
    "Re-arranging, we have \n",
    "\n",
    "$2 X^{T} X \\Theta  - 2 X^{T} y = 0$ which leads to \n",
    "\n",
    "\\begin{equation}\n",
    "X^{T} X \\Theta = X^{T} y\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve this linear system, we can use many different approaches. The matrix $X^T X$ will be positive symmetric definite, so we can use a variety of matrix solving techniques. Do NOT take the inverse of the matrix. Numerically this is a highly unstable approach. If using built in solvers in numpy, we have options. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is all, now let us make up some data and see if we can fit a linear regression model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make up a random data matrix with n data samples and d dimensions\n",
    "n = 1000\n",
    "d = 20 \n",
    "\n",
    "X = np.random.randn(n,d)\n",
    "\n",
    "#Make up a theta vector that we know ahead of time! We will use this to creat a ground truth y data set, \n",
    "#and then solve to see if we get the same value back\n",
    "theta = np.random.randn(d) \n",
    "\n",
    "#True data \n",
    "y_true = np.dot(X,theta)\n",
    "\n",
    "#Now find theta give y_true and X. We should get the same theta matrix we ahd already. \n",
    "\n",
    "left_hand_matrix = np.dot(np.transpose(X), X) \n",
    "right_hand_matrix = np.dot(np.transpose(X), y_true)\n",
    "\n",
    "theta_guess = np.linalg.solve(left_hand_matrix,right_hand_matrix)\n",
    "\n",
    "#Let's compare the difference of the given theta and the guessed theta. \n",
    "\n",
    "error = np.linalg.norm(theta-theta_guess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.61353233938522e-15\n"
     ]
    }
   ],
   "source": [
    "print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our error is within machine precision. Try this on a real life data set to see how good of a fit you can have. This example is simply a sanity check. Let's write a clean function that takes a data set $X$ and ground truth $y$ and gives us optimal weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(X,y):\n",
    "    left_hand_matrix = np.dot(X.transpose, X)\n",
    "    right_hand_matrix = np.dot(X.tranpose, y) \n",
    "    \n",
    "    theta = np.linalg.solve(left_hand_matrix, right_hand_matrix)\n",
    "    \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There it is, that simple!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
